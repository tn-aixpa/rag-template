{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e7a1652-e4b9-4e96-b3ce-0c52f199c884",
   "metadata": {},
   "source": [
    "## 1. Initialize project\n",
    "Create or use an existing project to scope the RAG scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8915e6ea-2cc7-4ba7-aca7-8d88c84f20dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import digitalhub as dh\n",
    "project = dh.get_or_create_project(\"rag-demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9d74fc-4c28-4538-9734-de640cd7a313",
   "metadata": {},
   "source": [
    "### 1.1. Prepare secrets\n",
    "For the project to work two secrets are needed:\n",
    "- ``HF_TOKEN``: to contain the HuggingFace token if a protected model should be downloaded from the HuggingFace\n",
    "- ``PG_CONN_URL``: full PGVector DB URL (with username/password) to connect to the corresponding vector store. The value may be obtained from the platform configuration or a new DB may be created from KRM with the necessary extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43798e3e-8c8e-4ea6-b6c8-36a1935b03d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "secret = project.new_secret(\"HF_TOKEN\", secret_value=\"<your value>\")\n",
    "pg_secret = project.new_secret(\"PG_CONN_URL\", secret_value=\"<full-pg-connection-url>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4117f5d2-2fb2-4cc8-8671-9047e4499c99",
   "metadata": {},
   "source": [
    "## 2. Deploy the supporting LLM for text generation\n",
    "Here we use the ``huggingfaceserve`` runtime for deploying the LLM service. Specifically, we deploy LLama3-8b-instruct model, but any HuggingFace-compatible model should work here. Alternatively, ``kubeai`` deployment may be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5d9a77-cdd9-4a7b-b898-d724dc0a6d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_func = project.new_function(\"chat\",\n",
    "                                    kind=\"huggingfaceserve\",\n",
    "                                    model_name=\"chatmodel\",\n",
    "                                    path=\"huggingface://meta-llama/meta-llama-3-8b-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056c1c33-7c93-42e3-9510-45792610806e",
   "metadata": {},
   "source": [
    "We run the function passing the specific execution properties:\n",
    "- ``profile``: execution profile for the node selection and resource usage (depends on the platform). In this example 1xa100 refers to 1 GPU of type A100.\n",
    "- ``max_length``: length of the context window for the text generation.\n",
    "- ``secrets``: list of secreets to pass to LLM. Needed if, e.g., HuggingFace token is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf04441-c1eb-4f21-b82a-18aad7f3cb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_run = chat_func.run(action=\"serve\",\n",
    "                           profile=\"1xa100\",\n",
    "                           max_length=\"5000\",\n",
    "                           secrets=[\"HF_TOKEN\"],\n",
    "                           wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb60e2e-bfef-44a4-9bbf-772ff737a3f7",
   "metadata": {},
   "source": [
    "Obtain the URL of the deployed service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70cc14c-b6fa-4511-b789-0302333e3ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_service_url = chat_run.refresh().status.to_dict()[\"service\"][\"url\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdccbe5-dbba-43de-a676-dc5fd1d37435",
   "metadata": {},
   "source": [
    "## 2. Deploy the supporting LLM for embeddings\n",
    "\n",
    "Embedding models map discrete data, such as words, to numerical vectors, which are more convenient for analysis, yet can still represent relationships between objects.    \n",
    "Here we use the ``huggingfaceserve`` runtime for deploying the embedding model service. Specifically, we deploy thenlper/gte-base, but any HuggingFace-compatible model should work here. Alternatively, ``kubeai`` deployment may be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4d24e8-80da-4d90-874f-fcab9f479428",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_func = project.new_function(\"emb\",\n",
    "                                kind=\"huggingfaceserve\",\n",
    "                                model_name=\"embmodel\",\n",
    "                                path=\"huggingface://thenlper/gte-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98a7f9a-f6fa-4c7b-af40-9373c121e4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_run = emb_func.run(action=\"serve\",\n",
    "                       huggingface_task=\"text_embedding\",\n",
    "                       wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7361a49c-4e1a-48f1-bcb0-2b226b9f4a23",
   "metadata": {},
   "source": [
    "Obtain the URL of the deployed service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7e3d7d-5587-4229-a2eb-a30571dec65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_service_url = emb_run.refresh().status.to_dict()[\"service\"][\"url\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d941f1-1127-474a-9fd9-6b122c9efbc9",
   "metadata": {},
   "source": [
    "## 3. Process the relevant information and store embeddings in the Vector storage\n",
    "\n",
    "In the RAG scenario a typical task is to store the supporting information into the Vector storage and use it later for the text generation. For this purpose, consider an example scenario, where the relevant information is first scrapped from a Web page URL and then stored into the platform using the provided PGVector storage. Two components are required:\n",
    "- Embeddings processor that uses Open Inference Protocol of our Embedding model service:\n",
    "  ```python\n",
    "    hf_embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
    "        api_key=\"ignore\",\n",
    "        api_url=f\"http://{os.environ[\"EMBEDDING_SERVICE_URL\"]}/v1/models/embmodel:predict\"\n",
    "    )\n",
    "    class CEmbeddings(HuggingFaceInferenceAPIEmbeddings):\n",
    "        def embed_documents(self, docs):\n",
    "            return hf_embeddings.embed_documents(docs)[\"predictions\"]\n",
    "\n",
    "    custom_embeddings = CEmbeddings(api_key=\"ignore\")\n",
    "  ```\n",
    "- PGVector storage from the platform:\n",
    "  ```python\n",
    "    vector_store = PGVector(\n",
    "        embeddings=custom_embeddings,\n",
    "        collection_name=\"my_docs\",\n",
    "        connection=os.environ[\"PG_CONN_URL\"],\n",
    "    )\n",
    "  ```\n",
    "\n",
    "We define a ``src/embedding.py`` Python Job to obtain the document, create chunks, and store their embeddings using the storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a57c785-89a2-4f1c-9a9c-dae76ebf4084",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_func = project.new_function(\"create_embeddings\", \n",
    "                                   kind=\"python\", \n",
    "                                   python_version=\"PYTHON3_10\",\n",
    "                                   code_src=\"src/embedding.py\",\n",
    "                                   handler=\"embed\",\n",
    "                                   requirements=[\"transformers==4.50.3\", \"psycopg_binary\", \"langchain-text-splitters\", \"langchain-community\", \"langgraph\", \"langchain-core\", \"langchain-huggingface\", \"langchain_postgres\", \"langchain[openai]\"]\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08df848-4b26-4755-8e22-6628ad76cb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_run = data_func.run(action=\"job\", \n",
    "                         parameters={\"url\": \"https://lilianweng.github.io/posts/2023-06-23-agent/\"},\n",
    "                         envs=[{\"name\": \"EMBEDDING_SERVICE_URL\", \"value\": embedding_service_url}],\n",
    "                         secrets=[\"PG_CONN_URL\"]\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5072b7-32dd-459c-9e9a-e068e455ae11",
   "metadata": {},
   "source": [
    "The results of the elaboration are stored in the corresponding database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f167f465-bf0c-4c83-9269-04d8a4b18fb6",
   "metadata": {},
   "source": [
    "## 4. Create RAG application API\n",
    "Once the components and data is in place we can create a LangChain-based application and expose it as API in the platform. This will use the chat model service, the vector database, and the serverless functionality.\n",
    "\n",
    "We create and deploy the serverless function that interacts with LLM and uses the vector store for retrieval. It uses a simple LangChaing graph composed out of two steps: retrieval and generation. The result of the generation is returned by the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d35ad63-b4dd-4d6a-b524-7214fd975a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve_func = project.new_function(\n",
    "    name=\"rag_service\", \n",
    "    kind=\"python\", \n",
    "    python_version=\"PYTHON3_10\", \n",
    "    code_src=\"src/serve.py\",     \n",
    "    handler=\"serve\",\n",
    "    init_function=\"init\",\n",
    "    requirements=[\"transformers==4.50.3\", \"psycopg_binary\", \"langchain-text-splitters\", \"langchain-community\", \"langgraph\", \"langchain-core\", \"langchain-huggingface\", \"langchain_postgres\", \"langchain[openai]\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34e72dd-b51f-4b2b-8309-50da8901166c",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve_run = serve_func.run(\n",
    "    action=\"serve\",\n",
    "    envs=[\n",
    "            {\"name\": \"EMBEDDING_SERVICE_URL\", \"value\": embedding_service_url},\n",
    "            {\"name\": \"CHAT_SERVICE_URL\", \"value\": chat_service_url}\n",
    "         ],\n",
    "    secrets=[\"PG_CONN_URL\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7869430-d804-47f9-b8d2-a52f554962c5",
   "metadata": {},
   "source": [
    "To test our API we make a call to the service endpoint providing a JSON with the example question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af4ea4a-46f2-4d1c-883c-d665e2d6ab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "serve_service_url = serve_run.refresh().status.to_dict()[\"service\"][\"url\"]\n",
    "\n",
    "res = requests.post(f\"http://{serve_service_url}\",json={\"question\": \"What is decomposition in LLM?\"})\n",
    "res.json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
